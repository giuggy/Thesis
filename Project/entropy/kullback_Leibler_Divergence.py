from scipy.stats import entropy
from sklearn.preprocessing import normalize
from math import log


def cleaner(p, q):
    res = []
    res1 = []
    for a, b in zip(p,q):
        if a != 0 and b != 0:
            res.append(a)
            res1.append(b)
    return res, res1

def norm(x):
    tot = sum(x)
    return list(map(lambda y: y/tot, x))

def kld (p, q):
    res = []
    for m, n in zip(p, q):
        r = m * log(m/n)
        res.append(r)
    return sum(res)

def main():
    p = [0.0, 0.0, 0.0, 0.0, 0.11570247933884298, 0.12563260232620083, 0.288135593220339, 0.3109337014916414, 0.10891089108910891, 0.12887751838823153, 1.3061224489795917, 1.0020746887966805, 0.96875, 7.56140350877193, 0, 1.0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.4224343675417661, 0, 0.37172278376508916, 0, 0, 0, 0, 0.5775656324582339, 0, 0.6282772162349108, 0, 0.5492857142857143, 0, 0.5709102320199266, 0, 0, 0, 0, 0.45071428571428573, 0, 0.42908976798007337, 0, 0.09712230215827339, 0, 0.07662968099861303, 0, 0, 0, 0, 0.9028776978417267, 0, 0.923370319001387, 0, 0.13920454545454544, 0, 0.146224554803871, 0, 0, 0, 0, 0.8607954545454546, 0, 0.853775445196129, 0]
    q = [0.10491803278688525, 0.11796032485687657, 1.3125, 1.0018999366687777, 0.2553763440860215, 0.2359046119759781, 0.924812030075188, 0.7743165924984107, 0.0, 0.0, 0.0, 0.0, 0, 0.4074074074074074, 0, 1.0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.2633663366336634, 0, 0.242998697945402, 0, 0, 0, 0, 0.7366336633663366, 0, 0.757001302054598, 0, 0.6365054602184087, 0, 0.6196814159292036, 0, 0, 0, 0, 0.36349453978159124, 0, 0.38031858407079644, 0, 0.09090909090909091, 0, 0.06814602219033286, 0, 0, 0, 0, 0.9090909090909091, 0, 0.9318539778096672, 0, 0.1359773371104816, 0, 0.13619504902101723, 0, 0, 0, 0, 0.8640226628895185, 0, 0.8638049509789828, 0]
    #p = [0] * 12
    p = [0.8, 0.3, 0.5, 0.2, 0.1]
    q = [0.1, 0.23, 0.15, 0.12, 0.21]

    p, q = cleaner(p, p)


if __name__ == '__main__':
    main()
